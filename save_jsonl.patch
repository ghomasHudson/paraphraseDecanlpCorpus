diff --git a/text/torchtext/datasets/generic.py b/text/torchtext/datasets/generic.py
index 3b15d70..32dc79d 100644
--- a/text/torchtext/datasets/generic.py
+++ b/text/torchtext/datasets/generic.py
@@ -23,6 +23,84 @@ CONTEXT_SPECIAL = 'Context:'
 QUESTION_SPECIAL = 'Question:'
 
 
+#****************************************************
+# Hooks to save as plain JSON
+
+def getStringProperties(obj):
+    '''Gets all properties of obj which are normal strings'''
+    goodProps = []
+    for prop in dir(obj):
+        if not prop.startswith('__') and prop not in ["answer","question","context","context_question"]:
+            val = getattr(obj,prop)
+            if not callable(val):
+                if True or (not type(val) == list):
+                    goodProps.append(prop)
+    return goodProps
+
+#hook into torch.save
+oldTorchSave = torch.save
+def tempTorchSave(obj,f):
+    '''Output to plain json before saving'''
+    #Do the origional torch save
+    oldTorchSave(obj,f)
+    extra_props = []
+    if type(obj) == tuple:
+        #if a tuple, find a list of examples
+        if type(obj) == dict:
+            extra_props = obj[1] #second positon is extra metadata
+        elif "woz" in f:
+            extra_props = [{"lang_dialogue_turn": o[0], "answer": o[1]} for o in obj[1]]
+        elif "srl" in f:
+            extra_props = [{"answerSplits": o} for o in obj[1]]
+        elif "squad" in f:
+            extra_props = [{"answerSplits": o} for o in obj[1]]
+            for i,e in enumerate(extra_props):
+                e["q_id"] = obj[2][i]
+        obj = obj[0]
+        '''
+        for item in obj:
+            if type(item) == list and type(item[0]) == data.Example:
+                obj = item
+        '''
+
+    if type(obj) == list and type(obj[0]) == data.Example:
+        #Convert to JSON
+        if type(f) == str:
+            with open(f+".jsonl",'w',encoding='utf-8') as jsonF:
+                for i,e in enumerate(obj,1):
+                    #find all the string properties of the example
+                    jsonOut = {}
+                    goodProps = getStringProperties(e)
+                    for prop in goodProps:
+                        propClean = prop.replace("Raw","")
+                        jsonOut[prop] = getattr(e,prop)
+                    if len(extra_props) > 0:
+                        for prop in extra_props[i-1].keys():
+                            jsonOut[prop] = extra_props[i-1][prop]
+                    jsonF.write(json.dumps(jsonOut))
+                    if i != len(obj):
+                        jsonF.write("\n")
+        else:
+            print("not file type")
+    else:
+        print("Not list of examples")
+torch.save = tempTorchSave
+
+#hook into data.Example.fromlist
+oldExampleFromList = data.Example.fromlist
+def tempExampleFromList(data, fields):
+    '''Add the raw text'''
+    example = oldExampleFromList(data, fields) #run existing method
+    #Add in the raw strings
+    example.contextRaw = data[0]
+    example.questionRaw = data[1]
+    example.answerRaw = data[2]
+    return example
+data.Example.fromlist = tempExampleFromList
+
+
+#****************************************************
+
 def get_context_question(context, question):
     return CONTEXT_SPECIAL +  ' ' + context + ' ' + QUESTION_SPECIAL + ' ' + question
 
@@ -34,7 +112,7 @@ class CQA(data.Dataset):
     @staticmethod
     def sort_key(ex):
         return data.interleave_keys(len(ex.context), len(ex.answer))
- 
+
 
 class IMDb(CQA, imdb.IMDb):
 
@@ -58,7 +136,7 @@ class IMDb(CQA, imdb.IMDb):
                     with open(fname, 'r') as f:
                         context = f.readline()
                     answer = labels[label]
-                    context_question = get_context_question(context, question) 
+                    context_question = get_context_question(context, question)
                     examples.append(data.Example.fromlist([context, question, answer, CONTEXT_SPECIAL, QUESTION_SPECIAL, context_question], fields))
                     if subsample is not None and len(examples) > subsample:
                         break
@@ -110,13 +188,13 @@ class SST(CQA):
                 for line in f:
                     parsed = list(csv.reader([line.rstrip('\n')]))[0]
                     context = parsed[-1]
-                    answer = labels[int(parsed[0])] 
-                    context_question = get_context_question(context, question) 
+                    answer = labels[int(parsed[0])]
+                    context_question = get_context_question(context, question)
                     examples.append(data.Example.fromlist([context, question, answer, CONTEXT_SPECIAL, QUESTION_SPECIAL, context_question], fields))
 
                     if subsample is not None and len(examples) > subsample:
                         break
-       
+
             os.makedirs(os.path.dirname(cache_name), exist_ok=True)
             print(f'Caching data to {cache_name}')
             torch.save(examples, cache_name)
@@ -174,7 +252,7 @@ class TranslationDataset(translation.TranslationDataset):
                     if src_line != '' and trg_line != '':
                         context = src_line
                         answer = trg_line
-                        context_question = get_context_question(context, question) 
+                        context_question = get_context_question(context, question)
                         examples.append(data.Example.fromlist([context, question, answer, CONTEXT_SPECIAL, QUESTION_SPECIAL, context_question], fields))
                         if subsample is not None and len(examples) >= subsample:
                             break
@@ -228,7 +306,7 @@ class SQuAD(CQA, data.Dataset):
                             question = ' '.join(qa['question'].split())
                             q_ids.append(qa['id'])
                             squad_id = len(all_answers)
-                            context_question = get_context_question(context, question) 
+                            context_question = get_context_question(context, question)
                             if len(qa['answers']) == 0:
                                 answer = 'unanswerable'
                                 all_answers.append(['unanswerable'])
@@ -242,10 +320,10 @@ class SQuAD(CQA, data.Dataset):
                                 all_answers.append([a['text'] for a in qa['answers']])
                                 #print('original: ', answer)
                                 answer_start = qa['answers'][0]['answer_start']
-                                answer_end = answer_start + len(answer) 
+                                answer_end = answer_start + len(answer)
                                 context_before_answer = context[:answer_start]
                                 context_after_answer = context[answer_end:]
-                                BEGIN = 'beginanswer ' 
+                                BEGIN = 'beginanswer '
                                 END = ' endanswer'
 
                                 tagged_context = context_before_answer + BEGIN + answer + END + context_after_answer
@@ -254,10 +332,10 @@ class SQuAD(CQA, data.Dataset):
                                 tokenized_answer = ex.answer
                                 #print('tokenized: ', tokenized_answer)
                                 for xi, x in enumerate(ex.context):
-                                    if BEGIN in x: 
+                                    if BEGIN in x:
                                         answer_start = xi + 1
                                         ex.context[xi] = x.replace(BEGIN, '')
-                                    if END in x: 
+                                    if END in x:
                                         answer_end = xi
                                         ex.context[xi] = x.replace(END, '')
                                 new_context = []
@@ -277,10 +355,10 @@ class SQuAD(CQA, data.Dataset):
                                     else:
                                         new_context.append(x)
                                 ex.context = new_context
-                                ex.answer = [x for x in ex.answer if len(x.strip()) > 0] 
+                                ex.answer = [x for x in ex.answer if len(x.strip()) > 0]
                                 if len(ex.context[answer_start:answer_end]) != len(ex.answer):
                                     import pdb; pdb.set_trace()
-                                ex.context_spans = list(range(answer_start, answer_end)) 
+                                ex.context_spans = list(range(answer_start, answer_end))
                                 indexed_answer = ex.context[ex.context_spans[0]:ex.context_spans[-1]+1]
                                 if len(indexed_answer) != len(ex.answer):
                                     import pdb; pdb.set_trace()
@@ -307,7 +385,7 @@ class SQuAD(CQA, data.Dataset):
             torch.save((examples, all_answers, q_ids), cache_name)
 
 
-        FIELD = data.Field(batch_first=True, use_vocab=False, sequential=False, 
+        FIELD = data.Field(batch_first=True, use_vocab=False, sequential=False,
             lower=False, numerical=True, eos_token=field.eos_token, init_token=field.init_token)
         fields.append(('context_spans', FIELD))
         fields.append(('answer_start', FIELD))
@@ -379,10 +457,10 @@ class Summarization(CQA, data.Dataset):
                 for line in lines:
                     ex = json.loads(line)
                     context, question, answer = ex['context'], ex['question'], ex['answer']
-                    context_question = get_context_question(context, question) 
+                    context_question = get_context_question(context, question)
                     ex = data.Example.fromlist([context, question, answer, CONTEXT_SPECIAL, QUESTION_SPECIAL, context_question], fields)
                     examples.append(ex)
-                    if subsample is not None and len(examples) >= subsample: 
+                    if subsample is not None and len(examples) >= subsample:
                         break
             os.makedirs(os.path.dirname(cache_name), exist_ok=True)
             print(f'Caching data to {cache_name}')
@@ -402,7 +480,7 @@ class Summarization(CQA, data.Dataset):
                 url_file_name = os.path.join(path, f'{cls.name}_wayback_{split}_urls.txt')
                 with open(url_file_name) as url_file:
                     for url in url_file:
-                        story_file_name = os.path.join(path, 'stories', 
+                        story_file_name = os.path.join(path, 'stories',
                             f"{hashlib.sha1(url.strip().encode('utf-8')).hexdigest()}.story")
                         try:
                             story_file = open(story_file_name)
@@ -415,7 +493,7 @@ class Summarization(CQA, data.Dataset):
                             with story_file:
                                 article, highlight = [], []
                                 is_highlight = False
-                                for line in story_file: 
+                                for line in story_file:
                                     line = line.strip()
                                     if line == "":
                                         continue
@@ -428,13 +506,13 @@ class Summarization(CQA, data.Dataset):
                                         highlight.append(line)
                                     else:
                                         article.append(line)
-                                example = {'context': unicodedata.normalize('NFKC', ' '.join(article)), 
-                                           'answer': unicodedata.normalize('NFKC', ' '.join(highlight)), 
+                                example = {'context': unicodedata.normalize('NFKC', ' '.join(article)),
+                                           'answer': unicodedata.normalize('NFKC', ' '.join(highlight)),
                                            'question': 'What is the summary?'}
                                 split_file.write(json.dumps(example)+'\n')
                                 collected_stories += 1
                                 if collected_stories % 1000 == 0:
-                                    print(example) 
+                                    print(example)
             print(f'Missing {missing_stories} stories')
             print(f'Collected {collected_stories} stories')
 
@@ -513,7 +591,7 @@ class WikiSQL(CQA, data.Dataset):
 
     def __init__(self, path, field, query_as_question=False, subsample=None, **kwargs):
         fields = [(x, field) for x in self.fields]
-        FIELD = data.Field(batch_first=True, use_vocab=False, sequential=False, 
+        FIELD = data.Field(batch_first=True, use_vocab=False, sequential=False,
             lower=False, numerical=True, eos_token=field.eos_token, init_token=field.init_token)
         fields.append(('wikisql_id', FIELD))
 
@@ -527,7 +605,7 @@ class WikiSQL(CQA, data.Dataset):
             expanded_path = os.path.expanduser(path)
             table_path = os.path.splitext(expanded_path)
             table_path = table_path[0] + '.tables' + table_path[1]
-           
+
             with open(table_path) as tables_file:
                 tables = [json.loads(line) for line in tables_file]
                 id_to_tables = {x['id']: x for x in tables}
@@ -548,8 +626,8 @@ class WikiSQL(CQA, data.Dataset):
                         question = human_query
                     else:
                         question = 'What is the translation from English to SQL?'
-                        context += f'-- {human_query}'  
-                    context_question = get_context_question(context, question) 
+                        context += f'-- {human_query}'
+                    context_question = get_context_question(context, question)
                     ex = data.Example.fromlist([context, question, answer, CONTEXT_SPECIAL, QUESTION_SPECIAL, context_question, idx], fields)
                     examples.append(ex)
                     all_answers.append({'sql': sql, 'header': header, 'answer': answer, 'table': table})
@@ -643,18 +721,18 @@ class SRL(CQA, data.Dataset):
                     t = ex['type']
                     aa = ex['all_answers']
                     context, question, answer = ex['context'], ex['question'], ex['answer']
-                    context_question = get_context_question(context, question) 
+                    context_question = get_context_question(context, question)
                     ex = data.Example.fromlist([context, question, answer, CONTEXT_SPECIAL, QUESTION_SPECIAL, context_question], fields)
                     examples.append(ex)
                     ex.squad_id = len(all_answers)
                     all_answers.append(aa)
-                    if subsample is not None and len(examples) >= subsample: 
+                    if subsample is not None and len(examples) >= subsample:
                         break
             os.makedirs(os.path.dirname(cache_name), exist_ok=True)
             print(f'Caching data to {cache_name}')
             torch.save((examples, all_answers), cache_name)
 
-        FIELD = data.Field(batch_first=True, use_vocab=False, sequential=False, 
+        FIELD = data.Field(batch_first=True, use_vocab=False, sequential=False,
             lower=False, numerical=True, eos_token=field.eos_token, init_token=field.init_token)
         fields.append(('squad_id', FIELD))
 
@@ -692,7 +770,7 @@ class SRL(CQA, data.Dataset):
 
                     new_example = True
                     for line in lines:
-                        line = line.strip() 
+                        line = line.strip()
                         if new_example:
                             context = cls.clean(line)
                             new_example = False
@@ -701,7 +779,7 @@ class SRL(CQA, data.Dataset):
                             new_example = True
                             continue
                         question, answers = line.split('?')
-                        question = cls.clean(line.split('?')[0].replace(' _', '') +'?') 
+                        question = cls.clean(line.split('?')[0].replace(' _', '') +'?')
                         answer = cls.clean(answers.split('###')[0])
                         all_answers = [cls.clean(x) for x in answers.split('###')]
                         if answer not in context:
@@ -750,9 +828,9 @@ class SRL(CQA, data.Dataset):
                             assert a in context
                             modified_all_answers.append(a)
                         split_file.write(json.dumps({'context': context, 'question': question, 'answer': answer, 'type': 'wiki', 'all_answers': modified_all_answers})+'\n')
-            
 
-            
+
+
 
     @classmethod
     def splits(cls, fields, root='.data',
@@ -794,10 +872,10 @@ class WinogradSchema(CQA, data.Dataset):
                 for line in f:
                     ex = json.loads(line)
                     context, question, answer = ex['context'], ex['question'], ex['answer']
-                    context_question = get_context_question(context, question) 
+                    context_question = get_context_question(context, question)
                     ex = data.Example.fromlist([context, question, answer, CONTEXT_SPECIAL, QUESTION_SPECIAL, context_question], fields)
                     examples.append(ex)
-                    if subsample is not None and len(examples) >= subsample: 
+                    if subsample is not None and len(examples) >= subsample:
                         break
             os.makedirs(os.path.dirname(cache_name), exist_ok=True)
             print(f'Caching data to {cache_name}')
@@ -817,13 +895,13 @@ class WinogradSchema(CQA, data.Dataset):
              splits = re.split(pattern, context)
              results = []
              for which_schema in range(2):
-                 vs = [v[which_schema] for v in variations] 
+                 vs = [v[which_schema] for v in variations]
                  context = ''
                  for idx in range(len(splits)):
                      context += splits[idx]
                      if idx < len(vs):
                          context += vs[idx]
-                 results.append(context) 
+                 results.append(context)
              return results
 
 
@@ -834,7 +912,7 @@ class WinogradSchema(CQA, data.Dataset):
                 if len(line.split()) == 0:
                     schemas.append(schema)
                     schema = []
-                    continue 
+                    continue
                 else:
                     schema.append(line.strip())
 
@@ -896,7 +974,7 @@ class WOZ(CQA, data.Dataset):
 
     def __init__(self, path, field, subsample=None, description='woz.en', **kwargs):
         fields = [(x, field) for x in self.fields]
-        FIELD = data.Field(batch_first=True, use_vocab=False, sequential=False, 
+        FIELD = data.Field(batch_first=True, use_vocab=False, sequential=False,
             lower=False, numerical=True, eos_token=field.eos_token, init_token=field.init_token)
         fields.append(('woz_id', FIELD))
 
@@ -911,12 +989,12 @@ class WOZ(CQA, data.Dataset):
                     ex = example_dict = json.loads(line)
                     if example_dict['lang'] in description:
                         context, question, answer = ex['context'], ex['question'], ex['answer']
-                        context_question = get_context_question(context, question) 
+                        context_question = get_context_question(context, question)
                         all_answers.append((ex['lang_dialogue_turn'], answer))
                         ex = data.Example.fromlist([context, question, answer, CONTEXT_SPECIAL, QUESTION_SPECIAL, context_question, woz_id], fields)
                         examples.append(ex)
 
-                    if subsample is not None and len(examples) >= subsample: 
+                    if subsample is not None and len(examples) >= subsample:
                         break
             os.makedirs(os.path.dirname(cache_name), exist_ok=True)
             print(f'Caching data to {cache_name}')
@@ -969,7 +1047,7 @@ class WOZ(CQA, data.Dataset):
                                                     prev_slot = previous_state['inform'][previous_state['inform'].index(slot)]
                                                     if prev_slot[1] != slot[1]:
                                                         delta_state['inform'].append(slot)
-                                            else: 
+                                            else:
                                                 delta_state['request'].append(slot[1])
                                                 current_state['request'].append(slot[1])
                                 previous_state = current_state
@@ -980,7 +1058,7 @@ class WOZ(CQA, data.Dataset):
                                 if len(delta_state['request']) > 0:
                                     answer += ' '
                                     answer += ', '.join(delta_state['request'])
-                                ex = {'context': ' '.join(context.split()), 
+                                ex = {'context': ' '.join(context.split()),
                                      'question': ' '.join(question.split()), 'lang': lang,
                                      'answer': answer if len(answer) > 1 else 'None',
                                      'lang_dialogue_turn': f'{lang}_{di}_{ti}'}
@@ -1027,10 +1105,10 @@ class MultiNLI(CQA, data.Dataset):
                     ex = example_dict = json.loads(line)
                     if example_dict['subtask'] in description:
                         context, question, answer = ex['context'], ex['question'], ex['answer']
-                        context_question = get_context_question(context, question) 
+                        context_question = get_context_question(context, question)
                         ex = data.Example.fromlist([context, question, answer, CONTEXT_SPECIAL, QUESTION_SPECIAL, context_question], fields)
                         examples.append(ex)
-                    if subsample is not None and len(examples) >= subsample: 
+                    if subsample is not None and len(examples) >= subsample:
                         break
             os.makedirs(os.path.dirname(cache_name), exist_ok=True)
             print(f'Caching data to {cache_name}')
@@ -1048,9 +1126,9 @@ class MultiNLI(CQA, data.Dataset):
             with open(os.path.expanduser(os.path.join(path, f'multinli_1.0_train.jsonl'))) as src_file:
                 for line in src_file:
                    ex = json.loads(line)
-                   ex = {'context': f'Premise: "{ex["sentence1"]}"', 
-                         'question': f'Hypothesis: "{ex["sentence2"]}" -- entailment, neutral, or contradiction?', 
-                         'answer': ex['gold_label'], 
+                   ex = {'context': f'Premise: "{ex["sentence1"]}"',
+                         'question': f'Hypothesis: "{ex["sentence2"]}" -- entailment, neutral, or contradiction?',
+                         'answer': ex['gold_label'],
                          'subtask': 'multinli'}
                    split_file.write(json.dumps(ex)+'\n')
 
@@ -1059,9 +1137,9 @@ class MultiNLI(CQA, data.Dataset):
                 with open(os.path.expanduser(os.path.join(path, 'multinli_1.0_dev_{}.jsonl'.format(subtask)))) as src_file:
                     for line in src_file:
                        ex = json.loads(line)
-                       ex = {'context': f'Premise: "{ex["sentence1"]}"', 
-                             'question': f'Hypothesis: "{ex["sentence2"]}" -- entailment, neutral, or contradiction?', 
-                             'answer': ex['gold_label'], 
+                       ex = {'context': f'Premise: "{ex["sentence1"]}"',
+                             'question': f'Hypothesis: "{ex["sentence2"]}" -- entailment, neutral, or contradiction?',
+                             'answer': ex['gold_label'],
                              'subtask': 'in' if subtask == 'matched' else 'out'}
                        split_file.write(json.dumps(ex)+'\n')
 
@@ -1105,11 +1183,11 @@ class ZeroShotRE(CQA, data.Dataset):
                 for line in f:
                     ex = example_dict = json.loads(line)
                     context, question, answer = ex['context'], ex['question'], ex['answer']
-                    context_question = get_context_question(context, question) 
+                    context_question = get_context_question(context, question)
                     ex = data.Example.fromlist([context, question, answer, CONTEXT_SPECIAL, QUESTION_SPECIAL, context_question], fields)
                     examples.append(ex)
 
-                    if subsample is not None and len(examples) >= subsample: 
+                    if subsample is not None and len(examples) >= subsample:
                         break
             os.makedirs(os.path.dirname(cache_name), exist_ok=True)
             print(f'Caching data to {cache_name}')
@@ -1137,8 +1215,8 @@ class ZeroShotRE(CQA, data.Dataset):
                            relation, question, subject, context = split_line[:4]
                            answer = ', '.join(split_line[4:])
                        question = question.replace('XXX', subject)
-                       ex = {'context': context, 
-                             'question': question, 
+                       ex = {'context': context,
+                             'question': question,
                              'answer': answer if len(answer) > 0 else 'unanswerable'}
                        split_file.write(json.dumps(ex)+'\n')
 
@@ -1230,18 +1308,18 @@ class OntoNotesNER(CQA, data.Dataset):
             examples = []
             with open(os.path.expanduser(path)) as f:
                 for line in f:
-                    example_dict = json.loads(line)  
+                    example_dict = json.loads(line)
                     t = example_dict['type']
                     a = example_dict['answer']
                     if (subtask == 'both' or t == subtask):
                         if a != 'None' or nones:
                             ex = example_dict
                             context, question, answer = ex['context'], ex['question'], ex['answer']
-                            context_question = get_context_question(context, question) 
+                            context_question = get_context_question(context, question)
                             ex = data.Example.fromlist([context, question, answer, CONTEXT_SPECIAL, QUESTION_SPECIAL, context_question], fields)
                             examples.append(ex)
 
-                    if subsample is not None and len(examples) >= subsample: 
+                    if subsample is not None and len(examples) >= subsample:
                         break
             os.makedirs(os.path.dirname(cache_name), exist_ok=True)
             print(f'Caching data to {cache_name}')
@@ -1271,14 +1349,14 @@ class OntoNotesNER(CQA, data.Dataset):
                            'QUANTITY': 'quantitative',
                            'ORDINAL': 'ordinal',
                            'CARDINAL': 'cardinal'}
-        
-        pluralize = {'person': 'persons', 'political': 'political', 'facility': 'facilities', 'organization': 'organizations', 
+
+        pluralize = {'person': 'persons', 'political': 'political', 'facility': 'facilities', 'organization': 'organizations',
                      'geopolitical': 'geopolitical', 'location': 'locations', 'product': 'products', 'event': 'events',
-                     'artwork': 'artworks', 'legal': 'legal', 'language': 'languages', 'date': 'dates', 'time': 'times', 
+                     'artwork': 'artworks', 'legal': 'legal', 'language': 'languages', 'date': 'dates', 'time': 'times',
                      'percentage': 'percentages', 'monetary': 'monetary', 'quantitative': 'quantitative', 'ordinal': 'ordinal',
                      'cardinal': 'cardinal'}
 
- 
+
         for split in [train, validation, test]:
             split_file_name = os.path.join(path, f'{split}.jsonl')
             if os.path.exists(split_file_name):
@@ -1299,7 +1377,7 @@ class OntoNotesNER(CQA, data.Dataset):
                             for line in lines:
                                 original = line
                                 line = cls.clean(line)
-                                entities = []  
+                                entities = []
                                 while True:
                                     start_enamex_open_idx = line.find('<ENAMEX')
                                     if start_enamex_open_idx == -1:
@@ -1307,13 +1385,13 @@ class OntoNotesNER(CQA, data.Dataset):
                                     end_enamex_open_idx = line.find('">') + 2
                                     start_enamex_close_idx = line.find('</ENAMEX>')
                                     end_enamex_close_idx = start_enamex_close_idx + len('</ENAMEX>')
-    
+
                                     enamex_open_tag = line[start_enamex_open_idx:end_enamex_open_idx]
                                     enamex_close_tag = line[start_enamex_close_idx:end_enamex_close_idx]
                                     before_entity = line[:start_enamex_open_idx]
                                     entity = line[end_enamex_open_idx:start_enamex_close_idx]
                                     after_entity = line[end_enamex_close_idx:]
-    
+
                                     if 'S_OFF' in enamex_open_tag:
                                         s_off_start = enamex_open_tag.find('S_OFF="')
                                         s_off_end = enamex_open_tag.find('">') if 'E_OFF' not in enamex_open_tag else enamex_open_tag.find('" E_OFF')
@@ -1321,7 +1399,7 @@ class OntoNotesNER(CQA, data.Dataset):
                                         enamex_open_tag = enamex_open_tag[:s_off_start-2] + '">'
                                         before_entity += entity[:s_off]
                                         entity = entity[s_off:]
-    
+
                                     if 'E_OFF' in enamex_open_tag:
                                         s_off_start = enamex_open_tag.find('E_OFF="')
                                         s_off_end = enamex_open_tag.find('">')
@@ -1329,8 +1407,8 @@ class OntoNotesNER(CQA, data.Dataset):
                                         enamex_open_tag = enamex_open_tag[:s_off_start-2] + '">'
                                         after_entity = entity[-s_off:] + after_entity
                                         entity = entity[:-s_off]
-    
-    
+
+
                                     label_start = enamex_open_tag.find('TYPE="') + len('TYPE="')
                                     label_end = enamex_open_tag.find('">')
                                     label = enamex_open_tag[label_start:label_end]
@@ -1339,7 +1417,7 @@ class OntoNotesNER(CQA, data.Dataset):
                                     offsets = (len(before_entity), len(before_entity) + len(entity))
                                     entities.append({'entity': entity, 'char_offsets': offsets, 'label': label})
                                     line = before_entity + entity + after_entity
-                                
+
                                 context = line.strip()
                                 is_no_good = False
                                 for entity_tuple in entities:
@@ -1352,26 +1430,26 @@ class OntoNotesNER(CQA, data.Dataset):
                                     print('Throwing out example that looks poorly labeled: ', original.strip(), ' (', file_id.strip(), ')')
                                     continue
                                 question = 'What are the tags for all entities?'
-                                answer = '; '.join([f'{x["entity"]} -- {label_to_answer[x["label"]]}' for x in entities]) 
+                                answer = '; '.join([f'{x["entity"]} -- {label_to_answer[x["label"]]}' for x in entities])
                                 if len(answer) == 0:
                                     answer = 'None'
-                                split_file.write(json.dumps({'context': context, 'question': question, 'answer': answer, 'file_id': file_id.strip(), 
+                                split_file.write(json.dumps({'context': context, 'question': question, 'answer': answer, 'file_id': file_id.strip(),
                                                              'original': original.strip(), 'entity_list': entities, 'type': 'all'})+'\n')
                                 partial_question = 'Which entities are {}?'
- 
+
                                 for lab, ans in label_to_answer.items():
                                     question = partial_question.format(pluralize[ans])
                                     entity_of_type_lab = [x['entity'] for x in entities if x['label'] == lab]
                                     answer = ', '.join(entity_of_type_lab)
                                     if len(answer) == 0:
                                         answer = 'None'
-                                    split_file.write(json.dumps({'context': context, 
-                                                                 'question': question, 
-                                                                 'answer': answer, 
-                                                                 'file_id': file_id.strip(), 
-                                                                 'original': original.strip(), 
-                                                                 'entity_list': entities, 
-                                                                 'type': 'one', 
+                                    split_file.write(json.dumps({'context': context,
+                                                                 'question': question,
+                                                                 'answer': answer,
+                                                                 'file_id': file_id.strip(),
+                                                                 'original': original.strip(),
+                                                                 'entity_list': entities,
+                                                                 'type': 'one',
                                                                  })+'\n')
 
 
@@ -1418,11 +1496,11 @@ class SNLI(CQA, data.Dataset):
                     example_dict = json.loads(line)
                     ex = example_dict
                     context, question, answer = ex['context'], ex['question'], ex['answer']
-                    context_question = get_context_question(context, question) 
+                    context_question = get_context_question(context, question)
                     ex = data.Example.fromlist([context, question, answer, CONTEXT_SPECIAL, QUESTION_SPECIAL, context_question], fields)
                     examples.append(ex)
 
-                    if subsample is not None and len(examples) >= subsample: 
+                    if subsample is not None and len(examples) >= subsample:
                         break
             os.makedirs(os.path.dirname(cache_name), exist_ok=True)
             print(f'Caching data to {cache_name}')
@@ -1443,8 +1521,8 @@ class SNLI(CQA, data.Dataset):
                 with open(os.path.expanduser(os.path.join(path, src_file_name))) as src_file:
                     for line in src_file:
                        ex = json.loads(line)
-                       ex = {'context': f'Premise: "{ex["sentence1"]}"', 
-                             'question': f'Hypothesis: "{ex["sentence2"]}" -- entailment, neutral, or contradiction?', 
+                       ex = {'context': f'Premise: "{ex["sentence1"]}"',
+                             'question': f'Hypothesis: "{ex["sentence2"]}" -- entailment, neutral, or contradiction?',
                              'answer': ex['gold_label']}
                        split_file.write(json.dumps(ex)+'\n')
 
@@ -1484,10 +1562,10 @@ class JSON(CQA, data.Dataset):
                 for line in lines:
                     ex = json.loads(line)
                     context, question, answer = ex['context'], ex['question'], ex['answer']
-                    context_question = get_context_question(context, question) 
+                    context_question = get_context_question(context, question)
                     ex = data.Example.fromlist([context, question, answer, CONTEXT_SPECIAL, QUESTION_SPECIAL, context_question], fields)
                     examples.append(ex)
-                    if subsample is not None and len(examples) >= subsample: 
+                    if subsample is not None and len(examples) >= subsample:
                         break
             os.makedirs(os.path.dirname(cache_name), exist_ok=True)
             print(f'Caching data to {cache_name}')
@@ -1498,7 +1576,7 @@ class JSON(CQA, data.Dataset):
     @classmethod
     def splits(cls, fields, name, root='.data',
                train='train', validation='val', test='test', **kwargs):
-        path = os.path.join(root, name) 
+        path = os.path.join(root, name)
 
         train_data = None if train is None else cls(
             os.path.join(path, 'train.jsonl'), fields, **kwargs)
